{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68e37b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import re\n",
    "import spacy \n",
    "import nltk \n",
    "import string\n",
    "import gensim \n",
    "import matplotlib.colors as mcolors\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('wordnet')\n",
    "en = spacy.load('en_core_web_sm')\n",
    "\n",
    "from langdetect import detect, detect_langs\n",
    "from deep_translator import GoogleTranslator\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from gensim import corpora\n",
    "from gensim.models import CoherenceModel\n",
    "from wordcloud import WordCloud, STOPWORDS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2150d348",
   "metadata": {},
   "outputs": [],
   "source": [
    "review_data = pd.read_csv('reviews.csv.gz', compression='gzip',\n",
    "                   error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59164654",
   "metadata": {},
   "source": [
    "Translating reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d73f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#detect languages of each review\n",
    "review_data['comments'] = review_data['comments'].astype('str')\n",
    "def det(x):\n",
    "    try:\n",
    "        lang = detect(x)\n",
    "    except:\n",
    "        lang = 'Other'\n",
    "    return lang\n",
    "\n",
    "review_data['Lang'] = review_data['comments'].apply(det)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e255380",
   "metadata": {},
   "outputs": [],
   "source": [
    "#translate reviews not in English\n",
    "def translation(rev, lan):\n",
    "    if lan != 'en':\n",
    "        try: \n",
    "            comment_translated = GoogleTranslator(source='auto', target='en').translate(rev)\n",
    "        except:\n",
    "            comment_translated = np.nan\n",
    "    else:\n",
    "        comment_translated = rev\n",
    "    return comment_translated\n",
    "                \n",
    "review_data['comments_translated'] = review_data.apply(lambda x: translation(x.comments, x.Lang), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e223a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop NA's and reset index\n",
    "review_data = review_data.dropna()\n",
    "review_data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee430f7",
   "metadata": {},
   "source": [
    "Sentiment scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09cae82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#obtain sentiment scores for all reviews\n",
    "analyser = SentimentIntensityAnalyzer()\n",
    "\n",
    "def pos_score(x):\n",
    "    score = analyser.polarity_scores(x)\n",
    "    return score['pos']\n",
    "\n",
    "review_data['positivity_score'] = review_data.apply(lambda x: pos_score(x['comments']), axis=1)\n",
    "\n",
    "def neg_score(x):\n",
    "    score = analyser.polarity_scores(x)\n",
    "    return score['neg']\n",
    "\n",
    "review_data['negativity_score'] = review_data.apply(lambda x: neg_score(x['comments']), axis=1)\n",
    "\n",
    "def neu_score(x):\n",
    "    score = analyser.polarity_scores(x)\n",
    "    return score['neu']\n",
    "\n",
    "review_data['neutral_score'] = review_data.apply(lambda x: neu_score(x['comments']), axis=1)\n",
    "\n",
    "def comp_score(x):\n",
    "    score = analyser.polarity_scores(x)\n",
    "    return score['compound']\n",
    "\n",
    "review_data['compound_score'] = review_data.apply(lambda x: comp_score(x['comments']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1361a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to pickle file\n",
    "review_data.to_pickle(\"reviews_vader_translated.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbd1932",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cc = review_data.drop(columns=['reviewer_name', 'comments_translated'])\n",
    "agg_review_scores = df_cc.groupby('listing_id').mean()\n",
    "agg_review_scores = agg_review_scores.drop(columns=['id'])\n",
    "agg_review_scores = agg_review_scores.drop(columns=['reviewer_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42de3d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_review_scores.to_pickle(\"agg_review_scores_translated.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5266f201",
   "metadata": {},
   "source": [
    "EDA Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc35f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#completely positive reviews\n",
    "review_data[review_data.positivity_score == 1].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7547386",
   "metadata": {},
   "outputs": [],
   "source": [
    "#distribution sentiment scores\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.figure(figsize=(16,5))\n",
    "plt.subplot(1,2,1)\n",
    "sns.distplot(review_data['positivity_score'])\n",
    "plt.subplot(1,2,2)\n",
    "sns.distplot(review_data['negativity_score'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd576f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#distribution sentiment scores\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.figure(figsize=(16,5))\n",
    "plt.subplot(1,2,1)\n",
    "sns.distplot(review_data['compound_score'])\n",
    "plt.subplot(1,2,2)\n",
    "sns.distplot(review_data['neutral_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a6f269",
   "metadata": {},
   "outputs": [],
   "source": [
    "#additional columns for length and number of words\n",
    "def len_rev(rev):\n",
    "    leng = len(rev)\n",
    "    return leng\n",
    "\n",
    "review_data['len_review']  = review_data.apply(lambda x: len_rev(x.loc['comments_translated']), axis=1)\n",
    "\n",
    "def words_no(rev):\n",
    "    words = rev.split()\n",
    "    return len(words)\n",
    "\n",
    "review_data['no_words']  = review_data.apply(lambda x: words_no(x.loc['comments_translated']), axis=1)\n",
    "review_data.describe()\n",
    "review_data = review_data.drop(columns=['no_words', 'len_review'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becea079",
   "metadata": {},
   "source": [
    "Review Recency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd44f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add column for recency\n",
    "from datetime import datetime\n",
    "today = datetime.today()\n",
    "\n",
    "def date_time(date):\n",
    "    new_date = datetime.strptime(date, '%Y-%m-%d')\n",
    "    days_tot = (today-new_date).days\n",
    "    return days_tot\n",
    "\n",
    "review_data['days_ago'] = review_data.apply(lambda x: date_time(x.loc['date']), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6947b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add column for new sentiment scores with weight for recency\n",
    "shortest_days = min(review_data['days_ago'])\n",
    "review_data['recency_weight'] = shortest_days/df_rt['days_ago']\n",
    "\n",
    "tot_rw = review_data.groupby('listing_id').sum()\n",
    "tot_rw.rename(columns ={'recency_weight':'sum_rec_weight'}, inplace = True)\n",
    "tot_rw = tot_rw['sum_rec_weight'].copy()\n",
    "\n",
    "review_data = pd.merge(review_data, tot_rw, on='listing_id')\n",
    "review_data['weighted_neg_score'] = review_data['negativity_score']*review_data['recency_weight']/review_data['sum_rec_weight'] #sum recency_weight per listing\n",
    "review_data['weighted_pos_score'] = review_data['positivity_score']*review_data['recency_weight']/review_data['sum_rec_weight'] #sum recency_weight per listing\n",
    "review_data['weighted_neutral_score'] = review_data['neutral_score']*review_data['recency_weight']/review_data['sum_rec_weight'] #sum recency_weight per listing\n",
    "review_data['weighted_comp_score'] = review_data['compound_score']*review_data['recency_weight']/review_data['sum_rec_weight'] #sum recency_weight per listing\n",
    "review_data = review_data.drop(columns=['id', 'reviewer_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a250adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add weighted sentiment scores per listing\n",
    "agg_weighted_rs = review_data.groupby('listing_id').mean()\n",
    "agg_weighted_rs = agg_weighted_rs[['weighted_neg_score', 'weighted_pos_score', 'weighted_neutral_score','weighted_comp_score']].copy()\n",
    "\n",
    "agg_weighted_rs.to_pickle(\"agg_weighted_rs.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1213e36a",
   "metadata": {},
   "source": [
    "Topic Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52282f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning reviews\n",
    "#lower case, remove punctuation, remove stopwords, lemmatization\n",
    "\n",
    "stop = set(en.Defaults.stop_words)\n",
    "exclude = set(string.punctuation)\n",
    "lemma = WordNetLemmatizer()\n",
    "def clean(doc):\n",
    "    doc = re.sub('<[^>]+>', '', doc)\n",
    "    doc = re.sub(\"Â´\", \"'\", doc)\n",
    "    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "    final = re.sub(' u ', ' ', normalized)\n",
    "    return final\n",
    "\n",
    "doc_clean = [clean(doc).split() for doc in review_data['comments_translated']] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79adf701",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the term dictionary of the corpus and converting corpus to document term matrix with dictionary\n",
    "dictionary = corpora.Dictionary(doc_clean)\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b8929f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating LDA model and training on document term matrix\n",
    "Lda = gensim.models.ldamodel.LdaModel\n",
    "ldamodel = Lda(doc_term_matrix, num_topics=6, alpha=0.3, eta=0.1, id2word = dictionary, passes=2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983fdd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#coherence scores for LDA Model\n",
    "coherencemodel = CoherenceModel(model=ldamodel, corpus=doc_term_matrix, dictionary=dictionary, coherence='u_mass')\n",
    "print(coherencemodel.get_coherence())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706bd994",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tune hyperparameters alpha and eta for each number of topics (change num_topics)\n",
    "\n",
    "a=[0.1,0.2,0.3, 0.4, 'symmetric']\n",
    "b= [0.01,0.1,0.2,'symmetric']\n",
    "num_topics = 9\n",
    "coherence_values = {}\n",
    "model_list = []\n",
    "for A in a:\n",
    "    for B in b:\n",
    "        model = gensim.models.ldamodel.LdaModel(corpus=doc_term_matrix, num_topics=num_topics, id2word=dictionary, passes=2, alpha=A, eta=B, random_state=1)\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, corpus=doc_term_matrix, dictionary=dictionary, coherence='u_mass')\n",
    "        coh=coherencemodel.get_coherence()\n",
    "        ab = str(A)+'+'+str(B)+':'+str(coh)\n",
    "        print(ab)\n",
    "        coherence_values[ab]= coh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295a6e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get topic probabilities per topic\n",
    "topics = num_topics\n",
    "topic_probs = []\n",
    "for i in range(len(doc_term_matrix)):\n",
    "    probs_i={}\n",
    "    for j in range(topics):\n",
    "        key = 'topic_'+str(j)\n",
    "        scores = ldamodel.__getitem__(doc_term_matrix[i], eps=None)\n",
    "        try:\n",
    "            topic_prob = scores[j][1]\n",
    "        except:\n",
    "            topic_prob = 0\n",
    "        probs_i[key]=topic_prob\n",
    "    topic_probs.append(probs_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd84756",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get aggregated topic probabilities per listing\n",
    "df_pr = pd.DataFrame(topic_probs)\n",
    "review_data = review_data.join(df_pr)\n",
    "agg_topics = review_data.groupby('listing_id').mean()\n",
    "agg_topics = agg_topics.drop(columns=['id', 'reviewer_id'])\n",
    "#change value '9t' for num_topics\n",
    "agg_topics.to_pickle(\"topic_modelling_values_9t.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5074f217",
   "metadata": {},
   "outputs": [],
   "source": [
    "#topics representative words\n",
    "all_topics = ldamodel.print_topics(num_topics=9, num_words=10)\n",
    "all_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ed8c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#wordcloud of Top N words in each topic\n",
    "##code based on https://www.machinelearningplus.com/nlp/topic-modeling-visualization-how-to-present-results-lda-models/#5.-Build-the-Topic-Model\n",
    "cols = [color for name, color in mcolors.TABLEAU_COLORS.items()] \n",
    "\n",
    "cloud = WordCloud(stopwords=stop,\n",
    "                  background_color='white',\n",
    "                  width=2500,\n",
    "                  height=1800,\n",
    "                  max_words=10,\n",
    "                  colormap='tab10',\n",
    "                  color_func=lambda *args, **kwargs: cols[i],\n",
    "                  prefer_horizontal=1.0)\n",
    "\n",
    "topics = ldamodel.show_topics(formatted=False)\n",
    "\n",
    "fig, axes = plt.subplots(3, 2, figsize=(10,10), sharex=True, sharey=True)\n",
    "\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    fig.add_subplot(ax)\n",
    "    topic_words = dict(topics[i][1])\n",
    "    cloud.generate_from_frequencies(topic_words, max_font_size=300)\n",
    "    plt.gca().imshow(cloud)\n",
    "    plt.gca().set_title('Topic ' + str(i), fontdict=dict(size=16))\n",
    "    plt.gca().axis('off')\n",
    "\n",
    "\n",
    "plt.subplots_adjust(wspace=0, hspace=0)\n",
    "plt.axis('off')\n",
    "plt.margins(x=0, y=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
